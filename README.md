# llms-write-vulnerable-code-they-can-detect
  Experimental framework proving LLMs can detect security vulnerabilities   with 97% accuracy but still generate the same vulnerable code when asked.    Tests SQL injection, XSS, and command injection across GPT-4, Claude,   Gemini, and other models.
